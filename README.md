## ğŸš€ Yerelde LLM Test Etmek Ä°Ã§in Pratik Bir Yol Arayanlara ğŸ‘‡

Kendi makinende **Ollama + Docker** stackâ€™i ile nasÄ±l 5+ aÃ§Ä±k kaynak bÃ¼yÃ¼k dil modelini (LLM) nasÄ±lÂ test edersin?

---

### Neden BÃ¶yle Bir AkÄ±ÅŸ Kurmaya DeÄŸer?

* ğŸ’¡ **Model KarÅŸÄ±laÅŸtÄ±rmasÄ±**Â â€”Â Mistral, Llamaâ€¯3, Qwen, Phiâ€‘mini ve Gemma gibi modelleri aynÄ± senaryoda Ã¶lÃ§Ã¼p hangisinin projenize uygun olduÄŸunu test edebilirsiniz.
* ğŸ› ï¸ **AraÃ§ & Åema UyumluluÄŸu**Â â€”Â Test betiÄŸi, fonksiyon Ã§aÄŸrÄ±sÄ± (toolÂ call) ve JSONâ€¯schema desteÄŸini de kontrol ediyor. Yani model sadece â€œsohbetâ€ edebiliyor mu deÄŸil; gerÃ§ek Ã¼retim akÄ±ÅŸÄ±na hazÄ±r mÄ±, onu Ã¶ÄŸreniyorsunuz.
* ğŸŒ± **Tamamen AÃ§Ä±k Kaynak**Â â€”Â Dockerfile,Â dockerâ€‘compose,Â Python betiÄŸi ve gereksinimler GitHubâ€™da. Ã‡ek, kur, Ã§alÄ±ÅŸtÄ±r.

---

### 0ï¸âƒ£ HazÄ±rlÄ±k

| Gereklilik                   | Detay                                    |
| ---------------------------- | ---------------------------------------- |
| **DockerÂ Desktop**           | Windowsâ€™ta WSLÂ 2 / Hyperâ€‘V aÃ§Ä±k olmalÄ±.  |
| **PythonÂ â‰¥â€¯3.10**            | Betik asenkron `httpx` kullanÄ±yor.       |
| **uv** (opsiyonel)           | BaÄŸÄ±mlÄ±lÄ±klarÄ± Ä±ÅŸÄ±k hÄ±zÄ±nda kurmak iÃ§in. |
| **NVIDIA ContainerÂ Toolkit** | GPU hÄ±zlandÄ±rma istiyorsan.              |

---

### 1ï¸âƒ£ Ollama Sunucusunu KaldÄ±r ğŸš¢

```bash
# modelleri **konteyner iÃ§inde** Ã§ekmek iÃ§in:
docker exec -it ollama ollama pull mistral:7b
docker exec -it ollama ollama pull llama3.2:latest
docker exec -it ollama ollama pull qwen3:14b
docker exec -it ollama ollama pull qwen3:8b
docker exec -it ollama ollama pull phi4-mini:3.8b-fp16
# opsiyonel
docker exec -it ollama ollama pull gemma3:12b
```

BittiÄŸinde \~10â€‘20â€¯GB arasÄ± depolama kullanÄ±yor, sonra tekrar indirmeye gerek yok.

---

### 3ï¸âƒ£ Python OrtamÄ± âš™ï¸

```bash
uv pip install httpx pydantic   # 2â€¯sn
# Bu iki paket, betikteki **httpx** ve **pydantic** dÄ±ÅŸÄ±ndaki tÃ¼m modÃ¼ller
# (asyncio, sys, json, datetime, random, typing) Python standart kÃ¼tÃ¼phanesidir.
# DolayÄ±sÄ±yla ek bir kurulum gerekmez.
```

Alternatif olarak klasik `pip`:

```bash
pip install httpx pydantic
```

---

### 4ï¸âƒ£ Testi Ã‡alÄ±ÅŸtÄ±r ğŸ§ª

```bash
uv run test_ollama_connection.py                # tÃ¼m modeller
uv run test_ollama_connection.py http://localhost:11434 llama3.2:latest  # tek model
```

BetiÄŸin sonunda ÅŸÃ¶yle bir tablo geliyor:

```
Model           | Basic | Tools | Schema | Combo
mistral:7b      | âœ…    | âœ…    | âœ…     | âœ…
qwen3:14b       | âœ…    | âŒ    | âœ…     | âŒ
...
```

Eksik kalan hÃ¼cre varsa betik ayrÄ±ntÄ±lÄ± hatayÄ± da listeliyor â€”Â debug sÃ¼reniz kÄ±salÄ±yor.

---

### 5ï¸âƒ£ KarÅŸÄ±laÅŸtÄ±ÄŸÄ±m KÃ¼Ã§Ã¼k PÃ¼rÃ¼zler

* **11434 portu meÅŸgul**Â â†’ eski konteyneri durdur/unpublish.
* **CUDA hatasÄ±**Â â†’ NVIDIAÂ Driver + Toolkit sÃ¼rÃ¼mÃ¼ uyumlu mu diye bak.
* **`uv`**\*\* bulunamadÄ±\*\*Â â†’ `pip install uv` ya da `cargo install uv` yeterli.

---

### Son SÃ¶z ğŸ¯

Yerelde LLM denemek iÃ§in saatlerce ortam kurmaya gerek yok. Bu akÄ±ÅŸla **10â€¯dakika iÃ§inde** model Ã§ek, test et, karar ver.

Beraber daha iyi test senaryolarÄ± geliÅŸtirebiliriz! HatalarÄ± iletmeyi unutmayÄ±n! ğŸš€

